<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorFlow学习(一)]]></title>
    <url>%2F2017%2F11%2F08%2F2017-11-08-untitled%2F</url>
    <content type="text"><![CDATA[Tensorflow 学习（一）综述TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组.一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话(session) 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象. 图 TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤被描述成一个图. 在执行阶段, 使用会话执行图中的 op. 构建图构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算.Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入. TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了. 1234567891011121314import tensorflow as tf# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点# 加到默认图中.## 构造器的返回值代表该常量 op 的返回值.matrix1 = tf.constant([[3., 3.]])# 创建另外一个常量 op, 产生一个 2x1 矩阵.matrix2 = tf.constant([[2.],[2.]])# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.# 返回值 'product' 代表矩阵乘法的结果.product = tf.matmul(matrix1, matrix2) 默认图现在有三个节点, 两个 constant() op, 和一个matmul() op. 为了真正进行矩阵相乘运算, 并得到矩阵乘法的 结果, 你必须在会话里启动这个图. 启动图启动图的第一步是创建一个 Session 对象, 如果无任何创建参数, 会话构造器将启动默认图. 123456789101112131415161718# 启动默认图.sess = tf.Session()# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. # 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回# 矩阵乘法 op 的输出.## 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.# # 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.## 返回值 'result' 是一个 numpy `ndarray` 对象.result = sess.run(product)print result# ==&gt; [[ 12.]]# 任务完成, 关闭会话.sess.close() Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作. 123with tf.Session() as sess: result = sess.run([product]) print result]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch官方教程代码翻译_ClassifyingNames_Charter-Level_RNN]]></title>
    <url>%2F2017%2F11%2F08%2FPytorch%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B%E4%BB%A3%E7%A0%81%E7%BF%BB%E8%AF%91_ClassifyingNames_Charter-Level_RNN%2F</url>
    <content type="text"><![CDATA[Pytorch官方教程代码翻译_ClassifyingNames_Charter-Level_RNN 我们将建立并训练一个基于caharacter-level RNN（个人理解：字符级的RNN模型）来分类单词，该模型将单词当做一串字母读入，在每一轮训练中输出预测结果和隐藏状态，将其以前的隐藏状态提供给下一步。我们将单词属于哪种语言作为最后的预测结果当做输出。 训练数据集采用来自18种语言的姓氏大概有1000多条。预测结果基于名字的拼写方式得出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# coding=utf-8from __future__ import unicode_literals,print_function,divisionfrom io import openimport globimport torchimport unicodedataimport string'''准备数据: 在数据集data/names目录下有18个TXT文件被命名为"[language].txt"，每个文件包含一串名字，每行一个 大多数名字都是罗马字符（但我们仍需要将Unicode转为ASCII码） 我们最终要得到一个将每种语言与其名字相对应的字典列表形式&#123;language:[names...]&#125; 变量 category 和 line （在例子中对应language和name)为以后方便扩展被使用'''def findFiles(path):return glob.glob(path)print(findFiles('data/names/*.txt'))all_letters = string.ascii_letters + " .,;'"n_letters = len(all_letters)#将Unicode转为ASCII码def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn' and c in all_letters )print(unicodeToAscii('Ślusàrski'))#建立category-line字典，每种语言与其名字相对应的字典列表形式&#123;language:[names...]&#125;category_lines = &#123;&#125;all_categories = []#读取文件进行行分割def readLines(filename): lines = open(filename,encoding='utf-8').read().strip().split('\n') return [unicodeToAscii(line) for line in lines]for filename in findFiles('data/names/*.txt'): category = filename.split('/')[-1].split('.')[0] all_categories.append(category) lines = readLines(filename) category_lines[category] = linesn_categories = len(all_categories)'''将名字转为Tensor: 我们需要将已经组织好的名字转为Tensor变量使得GPU和pytorch可以使用 '''def letterToIndex(letter): return all_letters.find((letter))def letterToTensor(letter): tensor = torch.zeros(1,n_letters) tensor[0][letterToIndex(letter)] = 1 return tensordef lineToTensor(line): tensor = torch.zeros(len(line),1,n_letters) for li,letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensorprint (letterToIndex('J'))print (lineToTensor('Jones').size()) 创建网络在自动求导梯度之前，在Torch创建一个RNN网络涉及到]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test_image]]></title>
    <url>%2F2017%2F11%2F07%2Ftest_image%2F</url>
    <content type="text"><![CDATA[test_image]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随记</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb学习笔记（二）]]></title>
    <url>%2F2017%2F11%2F07%2FMongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Mongodb学习笔记（二） 关于Unicode字符串的一点说明你可能已经注意到，之前存入数据库的事常规的Python字符串，这与我们从数据库服务器里取回来的看起来不同（比如 u’Mike’ 而不是‘Mike’）。 下面简单解释一下。 MongoDB 以格式保存数据. BSON 字符串都是 UTF-8编码的， 所以PyMongo必须确保它保存的字符串值包含有效地 UTF-8数据.常规字符串 ( )都是有效的，可以不改变直接保存。Unicode 字符串( )就需要先编码成 UTF-8 格式.例子里的字符串显示为u’Mike’ 而不是 ‘Mike’是因为 PyMongo 会把每个BSON 字符串转换成 Python 的unicode 字符串, 而不是常规的 str. 这个问题困扰了我好多天，其实并不用太担心，因为在python处理过程中它会依然正常处理 Mongodb查询返回值如果用find({})函数查询，则返回的是一个游标并不是字典，类似于list若要查看其中内容需用for循环遍历 123p = StockDB.get_collection('SHistA').find(&#123;&#125;,&#123;ticker:1,"_id":0&#125;)for doc in p: ... 用find_one({})返回的则是字典]]></content>
      <categories>
        <category>Mongodb</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mongodb</tag>
        <tag>pymongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F11%2F07%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树决策树以数据特征做划分，利用特征鲜明且完备的方式将数据划入不同的分类。是一种数值离散的聚类算法。 其中最主要的两个知识点是信息熵和信息增益。决策树根据所给数据特征的信息增益决定划分方式。 特征选择选取对训练数据具有分类功能的特征 信息熵 在信息论和概率统计中对随记变量不确定性的度量 设X是一个取有限个值的离散随机变量，其概率分布： $$P(X = x_i)=p_i, i = 1,2,···n$$ 则X的熵定义为：$H(X) =- \sum_{i=1}^{n}p_ilog(p_i)$log以2为底单位为比特（bit） 上式表明熵越大X的不确定度越大 若有二维随机变量(X,Y），其联合概率为：$$P(X = x_i,Y = yj) = p{ij} ， i = 1,2,3······n,j= 1,2,3······m$$ 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定度。$$H(Y|X) = \sum_{i = 1}^{n}p_iH(Y|X=x_i)$$ $$p_i = P(X = x_i),i = 1,2,3······n$$ 在得到一批数据后可以通过数据估计，所得熵与条件熵称经验熵和经验条件熵信息增益表示在得知特征X的条件下，而使得Y的信息不确定性减少的程度。 特征 X对训练数据集Y的信息增益g(Y,X)，定义为集合Y的经验熵H(Y)与特征 X给定条件下Y的经验条件熵H(Y|X)之差$$g(Y,X) = H(Y) - H(Y|X)$$ 因此对给定数据集和特征，信息增益越大的特征具有更强的分类能力 所以特征选择的方法：对数据集，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征，并迭代进行 计算信息熵（香农熵）12345678910111213141516# 计算信息熵 def CalcShannonEnt(dataSet) : # 计算数据集的输入个数 numEntries=len(dataSet) # []列表,&#123;&#125;元字典,()元组 labelCounts= &#123;&#125; # 创建存储标签的元字典 # 对数据集dataSet中的每一行featVec进行循环遍历 for featVec in dataSet : currentLabels=featVec[-1] # currentLabels为featVec的最后一个元素 if currentLabels not in labelCounts.keys() : # 如果标签currentLabels不在元字典对应的key中 labelCounts[currentLabels]=0 # 将标签currentLabels放到字典中作为key，并将值赋为0 labelCounts[currentLabels]+=1 # 将currentLabels对应的值加1 shannonEnt=0.0 # 定义香农熵shannonEnt for key in labelCounts : # 遍历元字典labelCounts中的key，即标签 prob=float(labelCounts[key]) / numEntries # 计算每一个标签出现的频率，即概率 shannonEnt -=prob * log(prob, 2)# 根据信息熵公式计算每个标签信息熵并累加到shannonEnt上return shannonEnt# 返回求得的整个标签对应的信息熵 计算条件熵选择最好的分类特征123456789101112131415161718def chooseBestFeatureToSplit(dataSet): # 选择使分割后信息增益最大的特征，即对应的列 numFeatures=len(dataSet[0]) - 1 # 获取特征的数目，从0开始，dataSet[0]是一条数据 baseEntropy=CalcShannonEnt(dataSet) # 计算数据集当前的信息熵 bestInfoGain=0.0 # 定义最大的信息增益 bestFeature=-1 # 定义分割后信息增益最大的特征 for i in range(numFeatures):# 遍历特征，即所有的列，计算每一列分割后的信息增益，找出信息增益最大的列 featList=[example[i] for example in dataSet] # 取出第i列特征赋给featList uniqueVals=set(featList) # 将特征对应的值放到一个集合中，使得特征列的数据具有唯一性 newEntropy=0.0 # 定义分割后的信息熵 for value in uniqueVals: # 遍历特征列的所有值(值是唯一的，重复值已经合并)，分割并计算信息增益 subDataSet=splitDataSet(dataSet,i, value) # 按照特征列的每个值进行数据集分割 prob=len(subDataSet) / float(len(dataSet)) # 计算分割后的每个子集的概率(频率) newEntropy+=prob * CalcShannonEnt(subDataSet) # 计算分割后的子集的信息熵并相加，得到分割后的整个数据集的信息熵 infoGain=baseEntropy - newEntropy # 计算分割后的信息增益 if (infoGain &gt; bestInfoGain): #如果分割后信息增益大于最好的信息增益 bestInfoGain=infoGain # 将当前的分割的信息增益赋值为最好信息增益 bestFeature=i # 分割的最好特征列赋为i return bestFeature # 返回分割后信息增益最大的特征列]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 学习笔记（一）]]></title>
    <url>%2F2017%2F11%2F07%2FMongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MongoDB 学习笔记（一）mongodb是一种非结构化存储数据库，它的数据是以文档为基础的在本地以二进制形式进行存储（Bosn），虽然有与sql查询语句相似的方式，但与SQL语法没有任何关系，更像是一种面相对象的编程，调用函数接口。 启动方式： 1234# apple @ admin [12:05:48] $ mongodb //启动一个mongodb服务器（默认端口在27017）,默认启动连接数据库为 ／data/db 若想指定数据库，需添加参数--dbpath 例： # apple @ admin [12:05:48] $ mongod --dbpath ~/workspace/stock/mongodb/data/db # apple @ admin [12:05:48] $ mongo //启动一个命令行交互程序，是mongodbd的一个进程 我在使用mongodb是主要用python编程进行数据录入和处理，因此使用mongodb提供的pymongo包在程序中连接数据库进行操作 连接数据库： 123from pymongo import MongoClient client=MongoClient() //创建一个服务器进程，默认自动创建端口 db = client['STOCK'] //连接到数据库STOCK 常用数据库操作增：函数名：insert_one() 作用：用于插入一个文档 使用格式：db.get_collection(‘CollectionName’).insert_one({‘key’:’value’}) 函数名：insert_many() 作用：用于插入多个文档 使用格式：db.get_collection(‘CollectionName’).insert_many([ {‘key’: i } ]for i in range(n)) 12345Acoll=db['SEquA'] Aresult=Acoll.insert_many( [ &#123; "ticker": SA['ticker'][i], "secShortName": SA['secShortName'][i]&#125;for i in range(len(SA))]) 查：函数名：find() 作用：查找所需要的文档 使用格式：db.get_collection(‘CollectionName’).find({‘key’:’value’}) 其他使用方式查看：http://api.mongodb.com/python/current/api/pymongo/collection.html?_ga=1.224606170.1159885722.1489465777#pymongo.collection.Collection.find 1234567ATicker=db.get_collection('SEquA').find( &#123;&#125;, &#123; 'ticker': 1, '_id':0&#125;) 注意：find()函数查找所返回的格式是dict,关于dict对象的操作方式自行查找改：函数名：update() 作用：对指定文档进行更新和添加 使用格式：db.get_collection(‘CollectionName’).update({filter},{operation}, otherparameter…) 12345678910111213db.get_collection("SConC").update( &#123; "c_name": SConC['c_name'][i]&#125;, &#123; "$set": &#123; SConC['code'][i]: &#123; "name": SConC['name'][i] &#125; &#125;&#125;, upsert=True)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mongodb</tag>
        <tag>pymongo</tag>
      </tags>
  </entry>
</search>
